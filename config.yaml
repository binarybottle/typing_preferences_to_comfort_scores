# typing_preferences_to_comfort_scores/config.yaml
# Configuration file for keyboard layout preference learning system, controlling the entire pipeline.
# Handles dataset configuration, feature selection, model parameters, recommendation generation,
# visualization, and logging settings. Supports five operational modes:
#   1. analyze_features: Determine feature importance thresholds
#   2. select_features: Select optimal feature combinations
#   3. recommend_bigram_pairs: Generate new data collection targets
#   4. train_model: Train on selected features with original/additional data
#   5. predict_bigram_scores: Generate bigram typing comfort predictions
#   6. predict_key_scores: Generate individual key typing comfort predictions
#   --no-split: Do not split data into train/test sets

#-----------------------------------------------------------------------
# Input data and key layout
#-----------------------------------------------------------------------
data:                                   # Path to processed data and additional training data
  input_file: "data/processed_bigram_data.csv"
  splits:
    test_ratio: 0.2                     # Proportion of data used for testing (0.2 = 20%)
    random_seed: 26                     # Seed for reproducible train/test splits
    split_data_file: #"data/split_train_test_indices.npz" # File to store train/test indices
  layout:
    chars: [q, w, e, r,                 # Characters in keyboard layout (half-QWERTY)
            a, s, d, f,                 # (only left-hand side used for symmetric analysis)
            z, x, c, v]

#-----------------------------------------------------------------------
# Feature selection
#-----------------------------------------------------------------------
# Select from among features not excluded in the analyze_features mode 
# (see "Excluded features" below; base features required for included interactions are retained).
feature_selection:
  importance_threshold: 0.00213         # Threshold for aligned effect importance (median importance from analyze_features mode)
  cross_validation:
    n_splits: 5                         # Number of CV folds for evaluating importance
    min_fold_size: 100                  # Minimum number of preferences per fold
  metrics_file: "output/data/feature_metrics.csv"   # File to store feature metrics
  model_file: "output/data/feature_selection_model.pkl"  # File to store feature selection model

features: 
  base_features: # (9)                  # Core features for analysis
    - rows_apart                        # Number of rows between keys
    - angle_apart                       # Angular distance between keys
    - sum_finger_values                 # Combined finger load values
    - sum_row_position_values           # Combined row position values
    - sum_engram_position_values        # Combined Engram position values
    - typing_time                       # Time taken to type bigram
    - adj_finger_diff_row               # Adjacent fingers in different rows
    - same_finger                       # Whether bigram uses same finger
    - outward_roll                      # Whether movement rolls outward
  interactions: []
 
  control_features:                       # Features to control for but not analyze
    - bigram_frequency                    # Bigram frequency in the English language

#-----------------------------------------------------------------------
# Model training
#-----------------------------------------------------------------------
# Settings to analyze features
__model_analysis: &feature_analysis
  chains: 4                            # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 3000                         # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 7000                      # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 15                    # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.95                    # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

# Settings to select features
# In the round-robin feature selection process, we're making relative comparisons between features to select 
# the best one at each round. Therefore high precision MCMC settings (chains, warmup, n_samples) are less critical.
# The model is trained on the selected features in the next step.
__model_selection: &feature_selection
  chains: 4                            # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 3000                         # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 7000                      # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 15                    # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.95                    # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

# Settings to train the model on the selected features
__model_slow: &model_train
  chains: 4                            # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 3000                         # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 7000                      # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 15                    # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.95                    # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

model:                                 # Bayesian model parameters (Stan)
  #<<: *feature_analysis               # Model feature analysis settings
  #<<: *feature_selection              # Model feature selection settings
  <<: *model_train                     # Model training settings
  feature_scale: 1.5                   # Scale parameter for feature weight priors (lower for more regularization)
  participant_scale: 0.5               # Scale parameter for participant effect priors (lower to reduce overfitting to individual participants)
  required_temp_mb: 18000              # Required temporary space in MB
  bigram_comfort_predictions_file: "output/data/estimated_bigram_scores.csv"  # File to store predicted bigram comfort scores
  key_comfort_predictions_file: "output/data/estimated_key_scores.csv"        # File to store predicted single-key comfort scores
  model_file: "output/data/bigram_score_prediction_model.pkl"                 # File to store trained model

#-----------------------------------------------------------------------
# Bigram pair recommendations
#-----------------------------------------------------------------------
recommendations:
    n_recommendations: 50                # Number of bigram pairs to recommend (limited by the proposed data collection population size)
    max_candidates: 5000                 # Maximum number of candidate pairs to evaluate
    weights:                             # Scoring weights (must sum to 1.0)
        information_gain: 0.618          # φ ≈ 0.618: Weight for model uncertainty reduction potential (the golden ratio provides a natural balance between two competing objectives)
        coverage_value: 0.382            # 1 - φ: Weight for feature space coverage improvement
    recommendations_file: "output/data/recommended_bigram_pairs.csv"    # File to store recommendations

#-----------------------------------------------------------------------
# Compute key scores from same-key analysis
#-----------------------------------------------------------------------
key_scoring:
  same_key_output_file: "output/data/key_scores_from_same_key.csv"
  min_comparisons: 5  # Minimum number of comparisons required for a key to have a valid score
  combine_with_model_scores: true  # Whether to combine these scores with model-based scores
  combination_weight: 0.5  # Weight given to same-key analysis when combining (0-1)

#-----------------------------------------------------------------------
# Visualization, logging, and paths
#-----------------------------------------------------------------------
visualization:
  dpi: 300                            # Resolution of saved plots (dots per inch)
  figure_size: [12, 8]                # Default figure dimensions (width, height)
  alpha: 0.6                          # Transparency for plot elements
  color_map: "viridis"                # Default colormap for visualizations (viridis is perceptually uniform)

logging:                              # Logging configuration
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"  # Standard Python logging format
  console_level: "INFO"               # Show INFO and above messages in console
  file_level: "DEBUG"                 # Store DEBUG and above messages in log file

paths:                                # Directory structure for outputs
  root_dir: "output"                  # Base directory for all outputs
  metrics_dir: "output/data"          # Directory for storing computed metrics
  plots_dir: "output/plots"           # Directory for saving visualizations
  logs_dir: "output/logs"             # Directory for log files




#-----------------------------------------------------------------------
# Included, excluded, and selected features
#-----------------------------------------------------------------------
### |
base_features: # (9)                  # Core features for analysis
    - rows_apart                        # Number of rows between keys
    - angle_apart                       # Angular distance between keys
    - sum_finger_values                 # Combined finger load values
    - sum_row_position_values           # Combined row position values
    - sum_engram_position_values        # Combined Engram position values
    - typing_time                       # Time taken to type bigram
    - adj_finger_diff_row               # Adjacent fingers in different rows
    - same_finger                       # Whether bigram uses same finger
    - outward_roll                      # Whether movement rolls outward

interactions above importance_threshold: # (19)
    - ['sum_finger_values', 'sum_row_position_values']
    - ['rows_apart', 'sum_finger_values', 'sum_row_position_values']
    - ['sum_engram_position_values', 'sum_finger_values']
    - ['angle_apart', 'sum_engram_position_values', 'sum_finger_values']
    - ['rows_apart', 'sum_engram_position_values', 'sum_finger_values']
    - ['rows_apart', 'sum_row_position_values']
    - ['rows_apart', 'same_finger']
    - ['angle_apart', 'sum_engram_position_values']
    - ['outward_roll', 'rows_apart', 'sum_row_position_values']
    - ['adj_finger_diff_row', 'sum_engram_position_values']
    - ['angle_apart', 'sum_finger_values']
    - ['adj_finger_diff_row', 'sum_engram_position_values', 'sum_finger_values']
    - ['rows_apart', 'sum_engram_position_values']
    - ['adj_finger_diff_row', 'rows_apart', 'sum_engram_position_values']
    - ['adj_finger_diff_row', 'angle_apart', 'sum_engram_position_values']
    - ['same_finger', 'sum_row_position_values']
    - ['adj_finger_diff_row', 'rows_apart', 'sum_row_position_values']
    - ['adj_finger_diff_row', 'sum_finger_values', 'sum_row_position_values']
    - ['adj_finger_diff_row', 'sum_row_position_values']

interactions below importance_threshold: # (23)
    - ['rows_apart', 'sum_finger_values']
    - ['adj_finger_diff_row', 'outward_roll', 'sum_engram_position_values']
    - ['outward_roll', 'rows_apart', 'sum_finger_values']
    - ['adj_finger_diff_row', 'rows_apart', 'sum_finger_values']
    - ['outward_roll', 'rows_apart']
    - ['adj_finger_diff_row', 'rows_apart']
    - ['outward_roll', 'rows_apart', 'sum_engram_position_values']
    - ['adj_finger_diff_row', 'outward_roll', 'sum_row_position_values']
    - ['angle_apart', 'outward_roll', 'sum_engram_position_values']
    - ['angle_apart', 'outward_roll', 'sum_finger_values']
    - ['outward_roll', 'sum_finger_values', 'sum_row_position_values']
    - ['outward_roll', 'sum_engram_position_values', 'sum_finger_values']
    - ['outward_roll', 'sum_row_position_values']
    - ['adj_finger_diff_row', 'outward_roll', 'rows_apart']
    - ['angle_apart', 'outward_roll']
    - ['outward_roll', 'sum_engram_position_values']
    - ['outward_roll', 'sum_finger_values']
    - ['adj_finger_diff_row', 'angle_apart', 'outward_roll']
    - ['adj_finger_diff_row', 'outward_roll', 'sum_finger_values']
    - ['adj_finger_diff_row', 'sum_finger_values']
    - ['adj_finger_diff_row', 'angle_apart']
    - ['adj_finger_diff_row', 'outward_roll']
    - ['adj_finger_diff_row', 'angle_apart', 'sum_finger_values']
###
